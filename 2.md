## 2. Decision trees

### 2.1 Classification trees (CART)
### 2.2 Regression trees (CART)
- [Piecewise constant function - Wikipedia](https://en.m.wikipedia.org/wiki/Step_function)
- It will be piecewise constant in some regions that are defined by the binary splits and on some higher dimensional space. (강의 내용) 
	- 예로든건 정의구역이 실수의 집합이라 “구간”(interval) 별로 상수인거구요, 정의구역이 $R^n$ 으로 되면, 예를 들어 이차원이면 (n=2) 사각형(rectangle) 별로 상수함수가 되겠죠.
### 2.3 Growing a regression tree (CART)
### 2.4 Growing a classification tree (CART)
- [CART - Breiman et.al](https://rafalab.github.io/pages/649/section-11.pdf)
- [CART - blog](https://machinelearningmastery.com/classification-and-regression-trees-for-machine-learning/)
- $E_R = \min_y {\frac 1 {N_R}} \sum_{i: x_i \in R} I(y_i \neq y)$ 에서 min이 붙는 이유는?
	- majority vote를 했을 때 정답이 아닌 경우의 전체 vote에서의 비율을 최소한으로 해서 같은 class끼리 같은 공간에 split 되도록 학습하기 위함입니다. 
### 2.5 Generalizations for trees (CART)
- $p$ explanatory variables $X_1, ... X_p$, $n$ observations가 있다고 가정했을 때 $X_i$는 1) a  numeric value ($n - 1$), 2) an ordered factor ($k - 1$), 3) an unordered factor ($2^{k-1} - 1$) 개만큼 predictor space가 나뉠 수 있다고 합니다. 왜 그럴까요?
	- 언오더드 팩터의 가지수가 전 좀 이해가 잘안가네요. 일단 숫자형 변수같은 경우는 처음에 스플릿하면서 분기를 찾아나갈 때 수치형 변수를 오름차순 또는 내림차순으로 정렬한후, 정렬된 변수에 대하여 데이터 별 변수값의 관측치로 스플릿을 해봅니다. 정렬된 변수의 첫번째값과 두번째값의 평균 또는 중앙값으로 스플릿을 해보고, 그다음 두번째값과 세번째깂의 평균 또는 중앙값으로 스플릿해봅니다. 이 과정을 n개의 값 대해서 다 시행하므로 n-1번입니다. 오더드 팩터같은 경우는 마찬가지로 오더드 팩터의 개수가 k개라면 같은 방식으로 생각해서 k-1개라고 할 수 있을 것 같습니다.
### 2.6 Bootstrap aggregation (Bagging)
- [Bootstrapping - Blog](https://learningcarrot.wordpress.com/2015/11/12/%EB%B6%80%ED%8A%B8%EC%8A%A4%ED%8A%B8%EB%9E%A9%EC%97%90-%EB%8C%80%ED%95%98%EC%97%AC-bootstrapping/)
- [Why Does Bagging Work? A Bayesian Account and its Implications - Pedro Domingos](https://homes.cs.washington.edu/~pedrod/papers/kdd97.pdf)
### 2.7 Bagging for classification
- [Bagging Predictors - Breiman](https://www.stat.berkeley.edu/~breiman/bagging.pdf)
- Bootstrapping에 대해서
	- 일단 저희가 보고 있는 데이터셋은 아무리 많다 한들 항상 무언가의 샘플입니다. 그리고 예를 들어 리그레션이라고 생각했을 때, 찾고자하는 모르는 직선이 있고, 그 직선을 따르는 점이 그대로 찍힌게 데이터가 아니라 생성되면서 뭔가 노이즈가 섞여있습니다.  이러한 노이즈가 섞인 눈에보이는 데이터로부터 저 모르는 선을 찾기위해 리니어 리그레션 또는 뉴럴넷을 쓸 수 있는데,  아시다 시피 뉴럴넷은 선을 그으면 뭔가 노이즈가 섞인 포인트도 다 이어 버릴 것입니다. 즉 이 말은 다른 샘플이 뽑히면 뉴럴넷이 찾은 어떤 선은 크게 달라질 수 있다는 얘기인데, 상대적으로 리니어 리그레션은 크게 달라지지 않습니다.  이는 모형에 분산이 존재한다는 의미이며, 이는 통계학에서 바이어스와 베리언스가 상충된다가 그대로 적용됩니다. 아나 모형쪽에서는 이런식으로 분해를 해보면 기저함수(찾고자 하는 모르는 선)에서 나오는 y와 모형(내가 선택한 뉴럴넷 , 리니어 리그레션 등의 방법)이 내주는 yhat의 차이의 제곱을  분해해보면 기저함수와 내가 선택한 모형의 차이 (바이어스) , 내가 선택한 모형의 변화정도 (베리언스), 설명할 수 없는 데이터 자체의 노이즈 이런식으로 분해됩니다.  ml에서는 바이어스 작고, 베리언스가 큰 모형은 모형의 capacity가 크다고 얘기합니다. 바이어스가 크고 베리언스가 작은 모형은 capacity가 작다고 얘기합니다.  바이어스와 베리언스는 상충되지만 바이어스를 키우지 않으면서, 베리언스를 줄일 수 있는 방법은 데이터를 늘리는 것입니다. 즉 배깅의 아이디어는 부트스트랩을 해서 각각의 부트스크랩 데이터로부터 모형을 만들어서, 그 모형의 결과를 종합하면 데이터를 늘려서 학습한 것과 마찬가지이므로 베리언스가 줄어든다라는 아이디어입니다.
### 2.8 Random forests
- [Birthday problem - Wikipedia](https://en.wikipedia.org/wiki/Birthday_problem)
