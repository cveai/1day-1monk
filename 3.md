## 3. Decision theory

### 3.1 Decision theory
### 3.2 Minimizing conditional expected loss
- 데이터 $x, y$를 왜 random variable로 가정할까?
	- "데이터"라는 말 자체가 벌써 현실에서의 무언가를 얘기하는 것이기 때문에 우리가 찾고자 하는 정답 즉 true $y = f(x)$ 가 있다고 가정하면, 데이터 $(x_1, y_1)$ 은 $y_1 = f(x_1)$를 만족하는 것이 아니고 $y_1 = f(x_1 + \epsilon_1) + \epsilon_2$ 라고 할 수 있겠죠. 근데 error($\epsilon$) 는 (확률적인) 분포로 줄 수 밖에 없는게 error 를 deterministic 하게 주면 사실 그건 error 가 아니죠. 하나 첨가하자면, 꼭 저렇게  RV 로 설정해야만 한다는 당위적인게 아니라  RV 로 놓고 확률적 접근을 했더니 일단 문제가 잘 정의되고,어느 정도 잘 해결도 되고 괜찮은 이론도 만들 수 있더라 뭐 이런 식이죠
### 3.3 Choosing f to minimize expected loss
### 3.4 Square loss
- [Smooth function - Wikipedia](https://en.wikipedia.org/wiki/Smoothness)
- 왜 squared loss를 계산할 때 $p(y|x)$를 smooth function으로 가정할까?
	- smooth function 얘기가 나오면 보통 두 가지를 의미할 수 있는데요 (1) 무한번 미분가능 (2) 적어도 한번 미분가능하고 도함수가 연속. 여기서는 편하게 충분히 미분가능하다고 가정하네요. loss의 최소값을 구하기 위해서 미분을 하기 때문에 이를 위해서 smooth를 가정합니다. 
### 3.5 The Big picture (part 1)
### 3.6 The Big picture (part 2)
### 3.7 The Big picture (part 3) 
